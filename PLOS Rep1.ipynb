{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6188abb1-f31f-4b72-9323-9964ec0d2bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SAVE_PATH = \"\"  # Data saving folder\n",
    "PREPROCESS = True  # If False, skip quantum processing and load data from SAVE_PATH\n",
    "np.random.seed(0)  # Seed for NumPy random number generator\n",
    "tf.random.set_seed(0)\n",
    "dataset_root = \"\"\n",
    "n_layers = 1  # Number of random layers\n",
    "\n",
    "def load_custom_dataset(root_folder, image_size=(100, 100)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_names = sorted(os.listdir(dataset_root))\n",
    "    class_map = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "    print(class_names)\n",
    "    for class_name in class_names:\n",
    "        class_folder = os.path.join(dataset_root, class_name)\n",
    "        if os.path.isdir(class_folder):\n",
    "            for image_name in os.listdir(class_folder):\n",
    "                image_path = os.path.join(class_folder, image_name)\n",
    "                try:\n",
    "                    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "                    image = cv2.resize(image, image_size)\n",
    "                    images.append(image)\n",
    "                    labels.append(class_map[class_name])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading image: {image_path}, Error: {e}\")\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "images, labels = load_custom_dataset(dataset_root)\n",
    "print(f\"Loaded {len(images)} images and {len(set(labels))} classes.\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "print(f\"Training samples: {X_train.shape[0]}, Testing samples: {X_test.shape[0]}\")\n",
    "n_train = X_train.shape[0]    # Size of the train dataset\n",
    "n_test = X_test.shape[0]     # Size of the test dataset\n",
    "train_images = X_train\n",
    "test_images = X_test\n",
    "train_images = train_images / 255\n",
    "test_images = test_images / 255\n",
    "train_images = np.array(train_images[..., np.newaxis])\n",
    "test_images = np.array(test_images[..., np.newaxis])\n",
    "\n",
    "def create_cnn_model(input_shape, num_classes):\n",
    "    \"\"\"Creates and returns a CNN model with backpropagation.\"\"\"\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        keras.layers.MaxPooling2D((2, 2)),\n",
    "        keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        keras.layers.MaxPooling2D((2, 2)),\n",
    "        keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "        keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def compute_metrics(model, test_images, test_labels):\n",
    "    y_pred = model.predict(test_images)\n",
    "    predicted_labels = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    # Compute metrics\n",
    "    precision = precision_score(test_labels, predicted_labels, average='weighted')\n",
    "    recall = recall_score(test_labels, predicted_labels, average='weighted')\n",
    "    f1 = f1_score(test_labels, predicted_labels, average='weighted')\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "# Define categories (labels) and input shape\n",
    "categories = [\"Class1\", \"Class2\", \"Class3\"]  # Update if different\n",
    "input_shape = (50, 50, 4)\n",
    "num_classes = len(categories)\n",
    "batch_size = 32\n",
    "n_epochs = 40\n",
    "\n",
    "# Load datasets\n",
    "datasets = {\n",
    "    'L1_CNOT': {\n",
    "        'train': np.load(SAVE_PATH + \"2x2_q_train_images_Apple_L1_CNOT_01082024_100.npy\"),\n",
    "        'test': np.load(SAVE_PATH + \"2x2_q_test_images_Apple_L1_CNOT_01082024_100.npy\"),\n",
    "    },\n",
    "    'L2_CNOT': {\n",
    "        'train': np.load(SAVE_PATH + \"2x2_q_train_images_Apple_L2_CNOT_01082024_100.npy\"),\n",
    "        'test': np.load(SAVE_PATH + \"2x2_q_test_images_Apple_L2_CNOT_01082024_100.npy\"),\n",
    "    },\n",
    "    'L3_CNOT': {\n",
    "        'train': np.load(SAVE_PATH + \"2x2_q_train_images_Apple_L3_CNOT_01082024_100.npy\"),\n",
    "        'test': np.load(SAVE_PATH + \"2x2_q_test_images_Apple_L3_CNOT_01082024_100.npy\"),\n",
    "    },\n",
    "    'L4_CNOT': {\n",
    "        'train': np.load(SAVE_PATH + \"2x2_q_train_images_Apple_L4_CNOT_01082024_100.npy\"),\n",
    "        'test': np.load(SAVE_PATH + \"2x2_q_test_images_Apple_L4_CNOT_01082024_100.npy\"),\n",
    "    },\n",
    "    'L5_CNOT': {\n",
    "        'train': np.load(SAVE_PATH + \"2x2_q_train_images_Apple_L5_CNOT_01082024_100.npy\"),\n",
    "        'test': np.load(SAVE_PATH + \"2x2_q_test_images_Apple_L5_CNOT_01082024_100.npy\"),\n",
    "    },\n",
    "    'L1_CZ': {\n",
    "        'train': np.load(SAVE_PATH + \"2x2_q_train_images_Apple_L1_CZ_01082024_100.npy\"),\n",
    "        'test': np.load(SAVE_PATH + \"2x2_q_test_images_Apple_L1_CZ_01082024_100.npy\"),\n",
    "    },\n",
    "    'L2_CZ': {\n",
    "        'train': np.load(SAVE_PATH + \"2x2_q_train_images_Apple_L2_CZ_01082024_100.npy\"),\n",
    "        'test': np.load(SAVE_PATH + \"2x2_q_test_images_Apple_L2_CZ_01082024_100.npy\"),\n",
    "    },\n",
    "    'L3_CZ': {\n",
    "        'train': np.load(SAVE_PATH + \"2x2_q_train_images_Apple_L3_CZ_01082024_100.npy\"),\n",
    "        'test': np.load(SAVE_PATH + \"2x2_q_test_images_Apple_L3_CZ_01082024_100.npy\"),\n",
    "    },\n",
    "    'L4_CZ': {\n",
    "        'train': np.load(SAVE_PATH + \"2x2_q_train_images_Apple_L4_CZ_01082024_100.npy\"),\n",
    "        'test': np.load(SAVE_PATH + \"2x2_q_test_images_Apple_L4_CZ_01082024_100.npy\"),\n",
    "    },\n",
    "    'L5_CZ': {\n",
    "        'train': np.load(SAVE_PATH + \"2x2_q_train_images_Apple_L5_CZ_01082024_100.npy\"),\n",
    "        'test': np.load(SAVE_PATH + \"2x2_q_test_images_Apple_L5_CZ_01082024_100.npy\"),\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define labels\n",
    "train_labels = y_train\n",
    "test_labels = y_test\n",
    "\n",
    "# Dictionary to store histories and test accuracies\n",
    "histories = {}\n",
    "test_accuracies = {}\n",
    "metrics = {}\n",
    "\n",
    "# Loop through models, train, and compute metrics\n",
    "for model_name in datasets.keys():\n",
    "    # Create and train model\n",
    "    model = create_cnn_model(input_shape=input_shape, num_classes=num_classes)\n",
    "    history = model.fit(\n",
    "        datasets[model_name]['train'], train_labels,\n",
    "        validation_data=(datasets[model_name]['test'], test_labels),\n",
    "        epochs=n_epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=2\n",
    "    )\n",
    "    histories[model_name] = history\n",
    "    test_loss, test_acc = model.evaluate(datasets[model_name]['test'], test_labels, verbose=0)\n",
    "    test_accuracies[model_name] = test_acc * 100  # Convert to percentage\n",
    "    \n",
    "    # Compute additional metrics\n",
    "    precision, recall, f1 = compute_metrics(model, datasets[model_name]['test'], test_labels)\n",
    "    metrics[model_name] = {\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Accuracy (%)': test_accuracies[model_name]\n",
    "    }\n",
    "\n",
    "# Create a DataFrame to tabulate the metrics\n",
    "metrics_df = pd.DataFrame(metrics).T  # Transpose for better readability\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d09ca4-2741-4136-aff5-21628b8cc3f5",
   "metadata": {},
   "source": [
    "MNIST Datset CNOT CZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89291930-fbcd-4026-a7c4-a7756af46133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist_dataset = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist_dataset.load_data()\n",
    "\n",
    "# Reduce dataset size\n",
    "n_train, n_test = 60000, 10000  # Modify as needed\n",
    "train_images = train_images[:n_train]\n",
    "train_labels = train_labels[:n_train]\n",
    "test_images = test_images[:n_test]\n",
    "test_labels = test_labels[:n_test]\n",
    "\n",
    "# Normalize pixel values\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0\n",
    "\n",
    "# Add extra dimension for convolution channels\n",
    "train_images = np.expand_dims(train_images, axis=-1)\n",
    "test_images = np.expand_dims(test_images, axis=-1)\n",
    "\n",
    "# Convert labels to categorical format\n",
    "num_classes = 10\n",
    "train_labels = to_categorical(train_labels, num_classes)\n",
    "test_labels = to_categorical(test_labels, num_classes)\n",
    "\n",
    "# Define dataset paths\n",
    "SAVE_PATH = \"D:/PhD_FUUAST/mail farina/quanvolution_PC/\"\n",
    "\n",
    "datasets = {\n",
    "    'M12_CNOT': {\n",
    "        'train': np.load(SAVE_PATH + \"q_train_images_M12.npy\"),\n",
    "        'test': np.load(SAVE_PATH + \"q_test_images_M12.npy\"),\n",
    "    },\n",
    "    'M12_CZ': {\n",
    "        'train': np.load(SAVE_PATH + \"q_train_images_M12_CZ.npy\"),\n",
    "        'test': np.load(SAVE_PATH + \"q_test_images_M12_CZ.npy\"),\n",
    "    },\n",
    "}\n",
    "\n",
    "# Dictionary to store histories and test accuracies\n",
    "histories = {}\n",
    "test_accuracies = {}\n",
    "metrics = {}\n",
    "\n",
    "def create_cnn_model(input_shape, num_classes):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n",
    "        keras.layers.MaxPooling2D((2,2)),\n",
    "        keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "        keras.layers.MaxPooling2D((2,2)),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "        keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def compute_metrics(model, test_data, test_labels):\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "    y_pred = np.argmax(model.predict(test_data), axis=1)\n",
    "    y_true = np.argmax(test_labels, axis=1)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Model Training Loop\n",
    "for model_name in datasets.keys():\n",
    "    print(f\"Training on {model_name} dataset...\")\n",
    "    \n",
    "    # Create and train model\n",
    "    model = create_cnn_model(input_shape=datasets[model_name]['train'].shape[1:], num_classes=num_classes)\n",
    "    \n",
    "    history = model.fit(\n",
    "        datasets[model_name]['train'], train_labels,\n",
    "        validation_data=(datasets[model_name]['test'], test_labels),\n",
    "        epochs=50,  # Adjust as needed\n",
    "        batch_size=128,\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    # Store history\n",
    "    histories[model_name] = history\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    test_loss, test_acc = model.evaluate(datasets[model_name]['test'], test_labels, verbose=0)\n",
    "    test_accuracies[model_name] = test_acc * 100  # Convert to percentage\n",
    "    \n",
    "    # Compute additional metrics\n",
    "    precision, recall, f1 = compute_metrics(model, datasets[model_name]['test'], test_labels)\n",
    "    metrics[model_name] = {\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Accuracy (%)': test_accuracies[model_name]\n",
    "    }\n",
    "\n",
    "# Convert to DataFrame for easy visualization\n",
    "metrics_df = pd.DataFrame(metrics).T\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbfb527-14f4-4066-803d-a3c18bf11a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## FRUITQ -SUBSET  ####################################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "SAVE_PATH = \"______________\"  # Data saving folder\n",
    "PREPROCESS = True  # If False, skip quantum processing and load data from SAVE_PATH\n",
    "np.random.seed(0)  # Seed for NumPy random number generator\n",
    "tf.random.set_seed(0)\n",
    "dataset_root = \"__________\"\n",
    "n_layers = 1  # Number of random layers\n",
    "\n",
    "def load_custom_dataset(root_folder, image_size=(100, 100)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_names = sorted(os.listdir(root_folder))\n",
    "    class_map = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "    print(class_names)\n",
    "\n",
    "    for class_name in class_names:\n",
    "        class_folder = os.path.join(root_folder, class_name)\n",
    "        if os.path.isdir(class_folder):\n",
    "            for image_name in os.listdir(class_folder):\n",
    "                image_path = os.path.join(class_folder, image_name)\n",
    "                try:\n",
    "                    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "                    if image is not None:\n",
    "                        image = cv2.resize(image, image_size)\n",
    "                        images.append(image)\n",
    "                        labels.append(class_map[class_name])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading image: {image_path}, Error: {e}\")\n",
    "\n",
    "    return np.array(images), np.array(labels), len(class_names)\n",
    "\n",
    "images, labels, num_classes = load_custom_dataset(dataset_root)\n",
    "print(f\"Loaded {len(images)} images and {num_classes} classes.\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "print(f\"Training samples: {X_train.shape[0]}, Testing samples: {X_test.shape[0]}\")\n",
    "\n",
    "# Normalize images\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Reshape for CNN input\n",
    "X_train = np.expand_dims(X_train, axis=-1)  # Add channel dimension\n",
    "X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "# Convert labels to categorical format\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Define dataset paths\n",
    "datasets = {\n",
    "    'M12_CNOT': {\n",
    "        'train': np.load(SAVE_PATH + \"2x2_q_train_images_Apple_L1_CNOT_FruitQ_100.npy\"),\n",
    "        'test': np.load(SAVE_PATH + \"2x2_q_test_images_Apple_L1_CNOT_FruitQ_100.npy\"),\n",
    "    },\n",
    "    'M12_CZ': {\n",
    "        'train': np.load(SAVE_PATH + \"2x2_q_train_images_Apple_L1_CZ_FruitQ_100.npy\"),\n",
    "        'test': np.load(SAVE_PATH + \"2x2_q_test_images_Apple_L1_CZ_FruitQ_100.npy\"),\n",
    "    },\n",
    "}\n",
    "\n",
    "# Dictionary to store histories and test accuracies\n",
    "histories = {}\n",
    "test_accuracies = {}\n",
    "metrics = {}\n",
    "\n",
    "def create_cnn_model(input_shape, num_classes):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n",
    "        keras.layers.MaxPooling2D((2,2)),\n",
    "        keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "        keras.layers.MaxPooling2D((2,2)),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "        keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def compute_metrics(model, test_data, test_labels):\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "    y_pred = np.argmax(model.predict(test_data), axis=1)\n",
    "    y_true = np.argmax(test_labels, axis=1)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Model Training Loop\n",
    "for model_name in datasets.keys():\n",
    "    print(f\"Training on {model_name} dataset...\")\n",
    "\n",
    "    # Ensure labels match dataset size\n",
    "    train_data, test_data = datasets[model_name]['train'], datasets[model_name]['test']\n",
    "    train_labels = y_train[:train_data.shape[0]]\n",
    "    test_labels = y_test[:test_data.shape[0]]\n",
    "\n",
    "    # Create and train model\n",
    "    model = create_cnn_model(input_shape=train_data.shape[1:], num_classes=num_classes)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_data, train_labels,\n",
    "        validation_data=(test_data, test_labels),\n",
    "        epochs=40,  # Adjust as needed\n",
    "        batch_size=128,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    # Store history\n",
    "    histories[model_name] = history\n",
    "\n",
    "    # Evaluate on test data\n",
    "    test_loss, test_acc = model.evaluate(test_data, test_labels, verbose=0)\n",
    "    test_accuracies[model_name] = test_acc * 100  # Convert to percentage\n",
    "\n",
    "    # Compute additional metrics\n",
    "    precision, recall, f1 = compute_metrics(model, test_data, test_labels)\n",
    "    metrics[model_name] = {\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Accuracy (%)': test_accuracies[model_name]\n",
    "    }\n",
    "\n",
    "# Convert to DataFrame for easy visualization\n",
    "metrics_df = pd.DataFrame(metrics).T\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac5faf9-501f-40e9-a917-8f0352d6e39f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
